# 100 Days Of ML - LOG

This challenge was proposed By [Siraj Raval](https://twitter.com/sirajraval).

## Day 1 : July 09 , 2018
 
**Today's Progress** : 

* Started by watching Siraj Raval's youtube tutorial [Learn Python for Data Science](https://www.youtube.com/playlist?list=PL2-dafEMk2A6QKz1mrk1uIGfHkC1zZ6UU). I've got the link to his tutorial from his repo [Learn_Machine_Learning_in_3_Months](https://github.com/llSourcell/Learn_Machine_Learning_in_3_Months) at the <b>Month 2 - Week 1</b> section.
* Created a new repo on github to test out the use of [TextBlob](https://textblob.readthedocs.io/en/dev/) and [tweepy](http://www.tweepy.org/) for sentiment analysis.

**Thoughts** : I passed a lot of time debugging errors like char encoding, file open mode (i was writing str and i opened the file with the binary option), tweepy rate limit exceeded and such. But I had fun working on this.

**What I learned**:

* How to create + fill a csv file.
* How to use tweepy (twitter python API).
* How to do a sentiment analysis using a bag of words based algorithm (Naive Bayes implemented in the TextBlob library).
* How to use re library (for regex on strings).

**Link of Work:**   [sentiment-analysis-twitter-textblob](https://github.com/touir1/sentiment-analysis-twitter-textblob) Github Repository.

## Day 2 : July 10 , 2018
 
**Today's Progress** : 

* Started watching a video by Siraj Raval on Recommendation Systems.
* Dropped watching that video and started a machine learning course from scratch by [Google](https://www.google.com) ([Introduction to machine learning](https://developers.google.com/machine-learning/crash-course/)). I stopped after finishing the <b>Descending into ML</b> section. 

**Thoughts** : It wasn't as easy as yesterday to understand the content of a video on machine learning. It had a lot of terms that I don't know so I prefered to start from scratch learning ML.

**What I learned**:

* Some basic [ML terminologies](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology).
* Basic [Linear Regression](https://developers.google.com/machine-learning/crash-course/descending-into-ml/linear-regression).
* A loss functions named [Mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error).
* How to use the [markdown](https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed) in the jupyter notebook.

**Link of Work:**   I passed the time reading so no work examples for today.

**TODO Next:**
- [X] Test out the linear regression algorithm with python(coded from scratch and using some ML libraries).
- [X] Try to represent data into graphs with python.
- [X] Learn how to animate the learning process of ML algorithms.

## Day 3 : July 11 , 2018
 
**Today's Progress** : 

* Watched a [video](https://www.youtube.com/watch?v=hY9Bc3mtphs) by Siraj Raval on the application of the Generative Adversarial Networks ([GAN](https://hackernoon.com/generative-adversarial-networks-a-deep-learning-architecture-4253b6d12347) for short) to discover new drugs.
* Studied some basics of machine learning like [Partial derivative](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivatives/v/partial-derivatives-introduction), [Gradient Descent](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent) and [Stochastic Gradient Descent (SGD)](https://developers.google.com/machine-learning/crash-course/reducing-loss/stochastic-gradient-descent).
* Started a new mini project consisting of implementing the [Linear Regression](https://developers.google.com/machine-learning/crash-course/descending-into-ml/linear-regression) algorithm using the SGD to reduce the loss.

**Thoughts** : I think that i'm going to choose a project where i'm going to applyML in the medical field. I like that idea. I found myself lacking the basics of calculus so I started watching videos + some tutorials to fill the gap.

**What I learned**:

* Some new terms like GAN and Recurrent Neural Network (RNN).
* How does the partial derivative work.
* How does the gradient descent work.

**Link of Work:**   [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github Repository.

**TODO Next:**
- [X] Implement linear regression with stochastic gradient descent from scratch.

## Day 4 : July 12 , 2018
 
**Today's Progress** : 

* Started reading how to implement linear regression in [GeeksforGeeks](https://www.geeksforgeeks.org/linear-regression-python-implementation/) website.
* Discovered that there is multiple types of regression. I need to study more the maths behind it.

**Thoughts** : I'm still far from having the basic knowledge for Machine learning. I think i'll study some maths before getting back to implementing algorithms.

**What I learned**: 

* Regression is a general term describing the process of estimating the relationships among variables.

**Link of Work:**  No coding for today.

**TODO Next:**
- [ ] Learn probability.
- [X] Learn calculus.

## Day 5 : July 13 , 2018
 
**Today's Progress** : 

* Started learning calculus. I'm using a youtube course by [3Blue1Brown](https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw) named [Essence of calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr).

**Thoughts** : It's not that hard to understand, i just need time to learn it.

**What I learned**: 

* Hard problem = SUM (small problems). [Youtube video](https://www.youtube.com/watch?v=WUvTyaaNkzM&t=0s&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr) used.
* What's a derivative. [Youtube video](https://www.youtube.com/watch?v=9vKqVkMQHKk&index=2&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr).

**Link of Work:**  No coding for today.

**TODO Next:**
- [X] Continue learning calculus.

## Day 6 : July 14 , 2018
 
**Today's Progress** : 

* Found a book on machine learning in a [Github repository](https://github.com/jakevdp/PythonDataScienceHandbook/tree/599aa0fe3f882c0001670e676e5a8d43b92c35fc). 
* Still watching the youtube course [Essence of calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr).. 

**Thoughts** : The book has some nice documentation about numpy, pandas and the visualisation.

**What I learned**: 

* Derivative formulas through geometry. [Youtube video](https://www.youtube.com/watch?v=S0_qX4VJhMQ&index=3&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr).

**Link of Work:**  No coding for today.

**TODO Next:**
- [X] Finish calculus course.
- [X] Start probability course. 

## Day 7 : July 15 , 2018
 
**Today's Progress** : 

* Still watching the youtube course [Essence of calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr). 

**Thoughts** : It's easier to visualize the derivative than to memorize it's formulas that's for sure.

**What I learned**: 

* Visualizing the chain rule and product rule. [Youtube video](https://www.youtube.com/watch?v=YG15m2VwSjA&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&t=28s&index=5).
* Derivatives of exponentials. [Youtube video](https://www.youtube.com/watch?v=m2MIpDrF7Es&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=5).
* Implicit differentiation. [Youtube video](https://www.youtube.com/watch?v=qb40J4N1fa4&index=6&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr).
* Limits. [Youtube video](https://www.youtube.com/watch?v=kfF40MiS7zA&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=7).

**Link of Work:**  No coding for today.

**TODO Next:**
- [X] Continue learning calculus.

## Day 8 : July 16 , 2018
 
**Today's Progress** : 

* Still watching the youtube course [Essence of calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr). 

**Thoughts** : Just a refresher about Integrals but from another perspective. I liked how the [mean of a function](https://en.wikipedia.org/wiki/Mean_of_a_function) was represented too.

**What I learned**: 

* Integrals and the fundamental theorem of calculus. [Youtube video](https://www.youtube.com/watch?v=rfG8ce4nNh0&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=8).
* What does area have to do with slope? [Youtube video](https://www.youtube.com/watch?v=FnJqaIESC2s&index=9&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr).

**Link of Work:** No coding for today.

**TODO Next:**
- [X] Continue learning calculus.

## Day 9 : July 17 , 2018
 
**Today's Progress** : 

* Finished watching the youtube course [Essence of calculus](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr).

**Thoughts** : Knowing how fast the function is changing using the second derivative is a pretty handy tip.

**What I learned**: 

* Higher order derivatives. [Youtube video](https://www.youtube.com/watch?v=BLkz5LGWihw&index=10&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr).
* Taylor series. [Youtube video](https://www.youtube.com/watch?v=3d6DsjIBzJ4&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=11).

**Link of Work:** No coding for today.

**TODO Next:**
- [X] Get back to the linear regression algorithm implementation.
- [X] Start probability course.

## Day 10 : July 18 , 2018
 
**Today's Progress** : 

* Got back to the linear regression implementation. I'm using a dataset from kaggle describing [Weather Conditions in World War Two](https://www.kaggle.com/smid80/weatherww2).

**Thoughts** : As always, passing a lot of time debugging and finding solution for data mapping like using the mean, groupby and reset_index of the [pandas](https://pandas.pydata.org/) library. 

**What I learned**: 

* How to read and clean the data with [pandas](https://pandas.pydata.org/).
* How to represent a nice plot using pandas for the mapping and [metaplotlib](https://matplotlib.org/) for the graph.

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [X] Implementing the loss function and the gradient descent.
- [X] Visualizing the output of the linear regression with [matplotlib](https://matplotlib.org/).

## Day 11 : July 19 , 2018
 
**Today's Progress** : 

* Finished implementing the simple linear regression algorithm from scratch.

**Thoughts** : I loved the statistics part of the simple linear regression. 

**What I learned**: 

* Some statistics functions like [Variance](https://en.wikipedia.org/wiki/Variance), [Covariance](https://en.wikipedia.org/wiki/Covariance) and [Mean](https://en.wikipedia.org/wiki/Mean).
* How to use [Latex](https://en.wikipedia.org/wiki/LaTeX) maths equations in jupyter notebook.
* How to minimize the [Root Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation).

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [X] Fix the Root Mean Squared Error function.
- [X] Implementing the simple linear regression using libraries.

## Day 12 : July 20 , 2018
 
**Today's Progress** : 

* Finished implementing the simple linear regression algorithm using [scikit-learn](http://scikit-learn.org/stable/).

**Thoughts** : I passed a lot of time trying to find a bug with the Series of pandas that didn't have indexes. Another thing, it's pretty easy to implement linear regression using scikit-learn. It doesn't need knowlegde about the algorithm or the logic behind it. I'm glad I tried to implement the linear regression from scratch first. 

**What I learned**: 

* How to use [scikit-learn](http://scikit-learn.org/stable/).
* How to split data into training and testing.

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [ ] Implementing the multivariable linear regression.

## Day 13 : July 21 , 2018
 
**Today's Progress** : 

* Began implementing the linear regression with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).

**Thoughts** : I like this jupyter notebook. It makes it easy to write a full document with mathematical expressions and running python code at the same time. 

**What I learned**: 

* Partial derivative of the [Mean Squared Error](https://en.wikipedia.org/wiki/Mean_squared_error) function.

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [X] Finish implementing the linear regression with [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent).
- [X] Animate the gradient descent with a nice graph.

## Day 14 : July 22 , 2018
 
**Today's Progress** : 

* Still implementing the linear regression gradient descent algorithm.

**Thoughts** : I'm still new with the [matplotlib](https://matplotlib.org/) library so i'm passing a lot of time understanding how it works. 

**What I learned**: 

* [Partial derivative](https://en.wikipedia.org/wiki/Partial_derivative) of the Mean Squared Error function.

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [X] Finish implementing the linear regression with gradient descent.
- [X] Animate the gradient descent with a nice graph.

## Day 15 : July 23 , 2018
 
**Today's Progress** : 

* Finished implementing the linear regression with gradient descent.

**Thoughts** : I need to learn more about [matplotlib](https://matplotlib.org/). We can create any sort of graph with it. 

**What I learned**: 

* How to animate the gradient descent algorithm.

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [X] Finish tweeking the gradient descent algorithm.
- [ ] Implement the gradient descent with python libraries like [scikit-learn](http://scikit-learn.org/stable/).

## Day 16 : July 24 , 2018
 
**Today's Progress** : 

* Passed the time optimising the gradient descent jupyter notebook code. I think the slowness is coming from my computer and not from the code but well, i'm still new so better be an optimised code.

**Thoughts** : I need to learn more about [matplotlib](https://matplotlib.org/). And also to learn how to get the best configuration for the algorithm (epochs, learning_rate etc).

**What I learned**: 

* Machine learning algorithms are very slow.
* Learned to use [pickle](https://docs.python.org/3/library/pickle.html) to save and load the data.
* Learned to use [logging](https://docs.python.org/3/library/logging.html) to log the learning process of ML algorithms.

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [X] Try to finish tweeking and optimising the gradient descent algorithm.

## Day 17 : July 25 , 2018
 
**Today's Progress** : 

* Passed the time debugging the code trying to figure out why it's slow and why the Mean Squared Error is going up.

**Thoughts** : It's easy to understand an algorithm but when it comes to implementation, that's were you'll have a hard time. But well, I learned a lot doing so and i'm intending to continue doing so.

**What I learned**: 

* Sometimes, a misplaced parameter in a function could get you a hedache just to find where the bug comes from. I need to pass the parameters with their names next.

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [X] Learn more about gradient descent.

## Day 18 : July 26 , 2018
 
**Today's Progress** : 

* I finished implementing the gradient descent and debugging it. And I searched the net to find out more about gradient descent.

**Thoughts** : There is much more things to learn about gradient descent and the learning rate. I never though that the parameters could be dynamic.

**What I learned**: 

* There is 3 types of gradient descent: One that uses all of the data (batch), one that uses one random value (stochastic) and the third uses a randomly selected values from the set (mini batch).
* There is algorithms to tweek the parameters (learning rate) of gradient descent dynamically.

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [X] Try to implement the three types of gradient descent and see the difference.

## Day 19 : July 27 , 2018
 
**Today's Progress** : 

* Passed the day studying about the gradient descent and it's different types and trying to implement them.
* Watched [Siraj Raval](https://twitter.com/sirajraval)'s [video](https://www.youtube.com/watch?v=suRd3UzdBeo) about a kaggle challenge and the use of [XGBoost](https://en.wikipedia.org/wiki/Xgboost).

**Thoughts** : It's pretty funny how we can jump from a subject to another. I found myself searching about [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter) optimal values using algorithms such as [Bayesian optimal hyperparameters tuning](https://arimo.com/data-science/2016/bayesian-optimization-hyperparameter-tuning/) while watching siraj's video. I didn't dig in further i'll put that as a goal for another time. 

**What I learned**: 

* We can use algorithms to tweek our hyperparameters.
* I think the optimal method for the gradient descent is the mini-batch which is a mix between the batch method and the stochastic method.

**Additional sources:**
** https://www.hackerearth.com/blog/machine-learning/3-types-gradient-descent-algorithms-small-large-data-sets/

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [ ] Learn more about the optimisation of hyperparameters.
- [X] Finish implementing the 3 gradient descent algorithms.

## Day 20 : July 28 , 2018
 
**Today's Progress** : 

* Finished implementing the 3 methods of gradient descent algorithm.

**Thoughts** : The stochastic approche is very sloppy i think. It's MSE values aren't very stable.

**What I learned**: 

* How to pin images with the markdown of a jupyter notebook.
* How to shuffle 2 arrays at the same time using zip.
* Learned more about the [matplotlib](https://matplotlib.org/) library.

**Link of Work:** [linear-regression-SGD](https://github.com/touir1/linear-regression-SGD) Github repository.

**TODO Next:**
- [ ] Implement the gradient descent using python libraries such as [scikit-learn](http://scikit-learn.org/stable/).

## Day 21 : July 29 , 2018
 
**Today's Progress** : 

* Got back to the [Machine learning course](https://www.udacity.com/course/intro-to-machine-learning--ud120) hosted by [Udacity](https://www.udacity.com/).

**Thoughts** : I forgot about a lot of things that i started before. One of them is this course of machine learning where I lastly implemented [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) algorithm using [scikit-learn](http://scikit-learn.org/stable/).

**What I learned**: 

* Machine learning is a vast field and i don't know from where to start.

**Link of Work:** No coding for today.

**TODO Next:**
- [ ] Implementing the naive bayes classifier from scatch with python.

## Day 22 : July 30 , 2018
 
**Today's Progress** : 

* Started the [naive bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) project but I passed the time learning more about naive bayes.

**Thoughts** : It's nice to see probalitics in action. After the implementation, I think i'll start the probability course.

**What I learned**: 

* Learned a little bit about [naive bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier).
* How to drop random rows respecting a condition in [pandas](https://pandas.pydata.org/) Dataframe.
* How to plot a pandas Dataframe directly.

**Link of Work:** [naive-bayes](https://github.com/touir1/naive-bayes) Github repository.

**TODO Next:**
- [ ] Finish up the naive bayes implementation from scratch.
- [X] Start learning probabilities.

## Day 23 : July 31 , 2018
 
**Today's Progress** : 

* I starded a [probability course](https://www.edx.org/course/introduction-probability-part-1-mitx-6-041-1x) on [edX](https://www.edx.org/) plateform.

**Thoughts** : probability is not that hard I think. Or maybe it's going to be harder if I dive deeper into the course.

**What I learned**: 

* Some axioms and derivatives in probability like the additivity, the union and such.

**Link of Work:** No coding for today.

**TODO Next:**
- [ ] Dive deeper into the probability course.

## Day 24 : August 01 , 2018
 
**Today's Progress** : 

* Still studying probability using this [probability course](https://www.edx.org/course/introduction-probability-part-1-mitx-6-041-1x) on [edX](https://www.edx.org/) plateform.

**Thoughts** : This course is more and more interesting. It's a complete probability course going through each notion in detail. I love it.

**What I learned**: 

* Studied some examples of probability (discrete and continuous) and learned about [countable additivity](https://www.statlect.com/glossary/countable-additivity).

**Link of Work:** No coding for today.

**TODO Next:**
- [X] Continue studying probability.

## Day 25 : August 02 , 2018
 
**Today's Progress** : 

* Still studying probability using this [probability course](https://www.edx.org/course/introduction-probability-part-1-mitx-6-041-1x) on [edX](https://www.edx.org/) plateform.

**Thoughts** : The course is now stating some basic knowledge to understand probability.

**What I learned**: 

* Some basic [Set](https://en.wikipedia.org/wiki/Set_(mathematics)) theory.
* Learned [De Morgan's laws](https://en.wikipedia.org/wiki/De_Morgan%27s_laws) for the complement of a union and a complement of a set.

**Link of Work:** No coding for today.

**TODO Next:**
- [X] Continue studying probability.

## Day 26 : August 03 , 2018
 
**Today's Progress** : 

* Still studying probability using this [probability course](https://www.edx.org/course/introduction-probability-part-1-mitx-6-041-1x) on [edX](https://www.edx.org/) plateform.

**Thoughts** : I didn't have time the past few days to to do big steps in the learning of probability but now, i'm free.

**What I learned**: 

* Some basic [Sequence](https://en.wikipedia.org/wiki/Sequence) theory like the convergence and limit of a sequence.

**Link of Work:** No coding for today.

**TODO Next:**
- [X] Continue studying probability.

## Day 27 : August 04 , 2018
 
**Today's Progress** : 

* Still studying probability using this [probability course](https://www.edx.org/course/introduction-probability-part-1-mitx-6-041-1x) on [edX](https://www.edx.org/) plateform.

**Thoughts** : The course is going to take a lot of time to finish. I think i'll do some coding and learning at the same time.

**What I learned**: 

* What's called [infinite series](https://www.mathsisfun.com/algebra/infinite-series.html).
* A quick reminder of [geometric series](https://en.wikipedia.org/wiki/Geometric_series).
* Learned about [countable](https://en.wikipedia.org/wiki/Countable_set) and [uncountable](https://en.wikipedia.org/wiki/Uncountable_set) sets.
* Learned about [Cantor's diagonalisation argument](https://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument). to prove that the set of real numbers is uncountable. 

**Link of Work:** No coding for today.

**TODO Next:**
- [X] Continue studying probability.
- [ ] Get back to coding.

## Day 28 : August 05 , 2018
 
**Today's Progress** : 

* Still studying probability using this [probability course](https://www.edx.org/course/introduction-probability-part-1-mitx-6-041-1x) on [edX](https://www.edx.org/) plateform.

**Thoughts** : [Venn diagram](https://en.wikipedia.org/wiki/Venn_diagram) is a nice tool to simplify a probability problem.

**What I learned**: 

* How to solve [Geniuses and chocolates](https://www.youtube.com/watch?v=y7lV5jwK27E) problem using [Venn diagram](https://en.wikipedia.org/wiki/Venn_diagram).
* Learned some uniform probabilities on a square in a video featuring [Romeo and Juliette](https://www.youtube.com/watch?v=jxxrwZtpHH0) problem to meet up.

**Link of Work:** No coding for today.

**TODO Next:**
- [X] Continue studying probability.

## Day 29 : August 06 , 2018
 
**Today's Progress** : 

* Still studying probability using this [probability course](https://www.edx.org/course/introduction-probability-part-1-mitx-6-041-1x) on [edX](https://www.edx.org/) plateform. Now i'm at the Unit 2 of the course (Conditioning and independence).

**Thoughts** : Bayes rule, here we go. I'm now studying conditionnal probability. next up is bayes rule i think.

**What I learned**: 

* The [Bonferroni's inequality](https://en.wikipedia.org/wiki/Boole%27s_inequality#Bonferroni_inequalities).
* The [conditionnal probabilities](https://en.wikipedia.org/wiki/Conditional_probability) definition.

**Link of Work:** No coding for today.

**TODO Next:**
- [ ] Learn Bayes rule.
